{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv2skd3-53rx"
   },
   "source": [
    "# Нейросети в задачах обработки текстов\n",
    "\n",
    "**Разработчик: Алексей Озерин, Ирина Сапарина**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mryab/dl-hse-ami/blob/master/week07_nlp/homework.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozLuJF3kIaPF"
   },
   "source": [
    "# Генерация коротких текстов с помощью Transformer\n",
    "\n",
    "\n",
    "Генерировать тексты можно как с помощью RNN, так и с помощью Transformer, предсказывая следующий символ последовательности по предыдущим. Мы будем использовать архитектуру Transformer.\n",
    "\n",
    "В этом задании предлагается написать и проучить на небольшом датасете имен генеративную модель на основе символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhrsBW44Q70a"
   },
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "!wget --quiet --show-progress \"https://raw.githubusercontent.com/mryab/dl-hse-ami/master/week07_nlp/names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_s_Z5lbIaPG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6nXxU8WIaPM"
   },
   "source": [
    "В файле `names` находится ~8k имен на латинице.\n",
    "\n",
    "Модель будет получать на вход имя `Amandy` и выдавать его же, только со сдвигом: `mandy `.\n",
    "\n",
    "Чтобы сеть училась генерировать заглавные буквы, добавим в начало специальный токен `_`.\n",
    "\n",
    "Также нам потребуется правило для останова генерации (это может быть просто ограничение на количество шагов). С другой стороны, можно добавить в конец каждого примера обучающей выборки специальный `<EOS>` токен. В данном случае обозначим его `#`:\n",
    "\n",
    "```\n",
    "_Amandy --> Amandy#\n",
    "```\n",
    "\n",
    "Можно прекращать генерацию при досрочном выпадании `<EOS>`.\n",
    "\n",
    "Для генерации на каждом шаге будем подавать на вход букву, предсказанную на предыдущем.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFRHva2zIaPN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "start_token = \"_\"\n",
    "eos = '#'\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.readlines()\n",
    "    names = [start_token + name.strip() + eos for name in names]\n",
    "\n",
    "names = list(set(names))  # в датасете есть повторы\n",
    "print('There are {} names: '.format(len(names)))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1 (0.05 балла):** постройте частоты употреблений букв в датасете. Для создания графика можно использовать функцию matplotlib.pyplot.bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSve0HBaIaPS"
   },
   "outputs": [],
   "source": [
    "<your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAeSKss4IaPV"
   },
   "outputs": [],
   "source": [
    "# в датасете есть слова с разными длинами\n",
    "MAX_LENGTH = max(map(len,names))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len,names)), bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWnDPWr9IaPY"
   },
   "outputs": [],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2 (0.1 балла):** отберите уникальные токены и заполните два словаря для конвертации токенов <-> индексы. Сделайте так, чтобы токен (`\"_\"`) (он же `start_token`) имел в словаре номер 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgB0VE9BIaPa"
   },
   "outputs": [],
   "source": [
    "tokens = <your code>\n",
    "    \n",
    "tok2id = <your code>\n",
    "id2tok = <your code>\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "print ('There are {} tokens',n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60\n",
    "\n",
    "print('Vocabulary: ' + \"\".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF_ukJotIaPd"
   },
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=tok2id[' '], dtype=np.int64):\n",
    "    \"\"\"Casts a list of names into matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(tok2id.get, names[i]))\n",
    "        names_ix[i, :len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmg_il5MIaPg"
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(names[:10]))\n",
    "print(to_matrix(names[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3 (0.05 балла):** разбейте все имена на тренировочную (80%) и тестовую часть (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64C8xOqCIaPk"
   },
   "outputs": [],
   "source": [
    "<your code>\n",
    "\n",
    "train_data, val_data = split_data(names)\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fW62jy6xIaPm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfGnm2QoIaPo"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhgqoEHOIaPr"
   },
   "source": [
    "# Char-Transformer для имен\n",
    "\n",
    "Вам нужно написать сеть, кодирующую входные символы и их позиции с помощью таблиц Embeddings. \n",
    "Получившиеся тензоры пропустить через `TransformerEncoder`, затем преобразовать в логиты для предсказания новых символов.\n",
    "\n",
    "Transformer может обрабатывать сразу всю последовательность за один проход. Для того, чтобы у модели не было возможности \"заглянуть в будущее\", то есть использовать информацию о впреди идущих символах, необходимо сгенерировать маску. `TransformerEncoder` должен принимать на вход последовательность символов и маску.    \n",
    "![Transformer](https://drive.google.com/uc?export=view&id=1gXILzT3mGgc0mGlvqY-6R4bGs3Lx2YxM)\n",
    "Картинка из [illustrated transformer](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4 (0.4 балла):** заполните все пропуски в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJCf0LYIIaPt"
   },
   "outputs": [],
   "source": [
    "class NameTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers=2, n_head=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        <your code>\n",
    "        \n",
    "        self.register_buffer(\"position_ids\", torch.arange(MAX_LENGTH).unsqueeze(1))\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, seq_len):\n",
    "        # TODO: сгенерируйте маску размера seq_len x seq_len\n",
    "        # если во время кодирования i-го символа j-й символ доступен, \n",
    "        # то (i,j) элемент маски равен 0, иначе -inf\n",
    "        \n",
    "        <your code>\n",
    "\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        <your code>\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmkgMHc8IaPu"
   },
   "source": [
    "# Код для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S164svO9IaPw"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_batches):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_batches:\n",
    "        \n",
    "        nums = to_matrix(batch)\n",
    "        <your code>\n",
    "            \n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log   \n",
    "\n",
    "def test(model, test_batches):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    for batch in test_batches:  \n",
    "        \n",
    "        nums = to_matrix(batch)\n",
    "        <your code>\n",
    "        \n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)    \n",
    "    points = np.array(val_history)\n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "    \n",
    "    bs = 32\n",
    "    total_steps = 0\n",
    "    train_batches = np.array_split(train_data, len(train_data) // bs)\n",
    "    test_batches = np.array_split(val_data, len(val_data) // bs)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, train_batches)\n",
    "        train_log.extend(train_loss)\n",
    "        total_steps += len(train_batches)\n",
    "        \n",
    "        val_loss = test(model, test_batches)\n",
    "        train_log.extend(train_loss)\n",
    "        \n",
    "        val_log.append((len(train_log), np.mean(val_loss)))\n",
    "        \n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxrc0a10IaPy"
   },
   "outputs": [],
   "source": [
    "model = NameTransformer(len(tokens), 64, 64, n_layers=2, n_head=2, dropout=0.1)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, opt, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffTAktWAIaP5"
   },
   "source": [
    "# Генерация по argmax\n",
    "**Задание 5 (0.2 балла):** реализуйте алгоритм «жадной» генерации последовательности с помощью argmax, заполнив все пропуски в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugRlkX2ZIaP6"
   },
   "outputs": [],
   "source": [
    "# Напишите функцию генерации продолжения строки\n",
    "def pick_by_argmax(logits):\n",
    "    <your code>\n",
    "\n",
    "def ids2string(ids):\n",
    "    return \"\".join(id2tok[_] for _ in ids)\n",
    "\n",
    "\n",
    "def gen_continuation(model, prefix=\"_\"):\n",
    "    nums = to_matrix(prefix)\n",
    "    nums = torch.from_numpy(nums)\n",
    "    \n",
    "    # TODO: сначала подайте на вход префикс\n",
    "    # нас интересует последний output, чтобы получить первое предсказание\n",
    "    <your code>\n",
    "    \n",
    "    # TODO: затем сгенерируйте несколько последующих символов\n",
    "    # outs -- это массив с номерами токенов\n",
    "    <your code>\n",
    "    \n",
    "    print(prefix + '|'+ ids2string(outs))\n",
    "    \n",
    "gen_continuation(model, \" Ku\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00547AA-IaP_"
   },
   "source": [
    "# Генерация с семплированием\n",
    "\n",
    "Обычный softmax \n",
    "$$p_i = \\frac{\\exp (x_i)}{\\sum \\exp (x_j)}$$\n",
    "можно модифицировать с помощью температуры:\n",
    "$$p_i = \\frac{\\exp (x_i / T)}{\\sum \\exp (x_j / T)}$$\n",
    "\n",
    "Это позволит плавно переходить от выбора наиболее вероятного элемента ($T << 1$) до практически равновероятного ($T >> 1$)\n",
    "\n",
    "**Задание 6 (0.2 балла):** реализуйте алгоритм семплирования с температурой, заполнив все пропуски в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71cOcFxpIaQA"
   },
   "outputs": [],
   "source": [
    "# Напишите функцию генерации батчами с семплированием из распределения и температурой\n",
    "def batch2string(ids, prefix):\n",
    "    # модифицируйте ids2string для работы с батчами\n",
    "    <your code>\n",
    "\n",
    "def pick_by_distribution(logits):\n",
    "    # превратите логиты в распределение\n",
    "    # затем семлируйте из него batch примеров\n",
    "    <your code>\n",
    "\n",
    "\n",
    "def gen_continuation_temp(model, prefix=\"_\", temperature=1.0, n=10):\n",
    "    nums = to_matrix([prefix] * n)\n",
    "    nums = torch.from_numpy(nums)\n",
    "\n",
    "    # аналогично, сначала подайте на вход префикс\n",
    "    # нас интересует последний output, чтобы получить первое предсказание\n",
    "    <your code>\n",
    "    \n",
    "    # затем, сгенерируйте n последующих символов\n",
    "    # в outs положите матрицу номеров токенов и отобразите ее\n",
    "    \n",
    "    print(batch2string(outs, prefix + '|'))\n",
    "\n",
    "gen_continuation_temp(model, prefix=\" An\", temperature=0.5, n=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL20-fall-seminar6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
